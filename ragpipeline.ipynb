{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933df8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\savin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\savin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\savin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\savin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement SentenceTransformer (from versions: none)\n",
      "ERROR: No matching distribution found for SentenceTransformer\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install arxiv pymupdf sentence-transformers faiss-cpu SentenceTransformer tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e7ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\savin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pymupdf\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm #Progress bar\n",
    "import pickle #For saving and loading the FAISS index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e83725",
   "metadata": {},
   "source": [
    "**Sentence-Transformers:** It is a populor open-source framework by Hugging Face community designed to make semantic embeddings. These embeddings capture semantic meaning rather than just word-level similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8f79e",
   "metadata": {},
   "source": [
    "## Data Collection:\n",
    "\n",
    "Retriving the most recent papers from arXiv in cs.CL catagoriy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savin\\AppData\\Local\\Temp\\ipykernel_22356\\295342325.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = search.results()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 20 papers in category cs.CL...\n",
      "1. Title: Searching for Privacy Risks in LLM Agents via Simulation downloading ... ... Done\n",
      "2. Title: A Survey on Diffusion Language Models downloading ... ... Done\n",
      "3. Title: SSRL: Self-Search Reinforcement Learning downloading ... ... Done\n",
      "4. Title: From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms downloading ... ... Done\n",
      "5. Title: Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning downloading ... ... Done\n",
      "6. Title: Reinforced Language Models for Sequential Decision Making downloading ... ... Done\n",
      "7. Title: Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions downloading ... ... Done\n",
      "8. Title: Beyond \"Not Novel Enough\": Enriching Scholarly Critique with LLM-Assisted Feedback downloading ... ... Done\n",
      "9. Title: Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models downloading ... ... Done\n",
      "10. Title: Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs downloading ... ... Done\n",
      "11. Title: Learning from Natural Language Feedback for Personalized Question Answering downloading ... ... Done\n",
      "12. Title: Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph downloading ... ... Done\n",
      "13. Title: Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages downloading ... ... Done\n",
      "14. Title: eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM downloading ... ... Done\n",
      "15. Title: When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models downloading ... ... Done\n",
      "16. Title: Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards downloading ... ... Done\n",
      "17. Title: Improving Value-based Process Verifier via Low-Cost Variance Reduction downloading ... ... Done\n",
      "18. Title: Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment downloading ... ... Done\n",
      "19. Title: Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model downloading ... ... Done\n",
      "20. Title: When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing downloading ... ... Done\n",
      "Paper list saved to paperlist.json\n",
      "Download completed!\n",
      "[{'paper_title': 'Searching for Privacy Risks in LLM Agents via Simulation', 'filename': 'papers/paper_1.pdf', 'summary': 'The widespread deployment of LLM-based agents is likely to introduce a\\ncritical privacy threat: malicious agents that proactively engage others in\\nmulti-turn interactions to extract sensitive informat'}, {'paper_title': 'A Survey on Diffusion Language Models', 'filename': 'papers/paper_2.pdf', 'summary': 'Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\\npromising alternative to the dominant autoregressive (AR) paradigm. By\\ngenerating tokens in parallel through an iterative denois'}, {'paper_title': 'SSRL: Self-Search Reinforcement Learning', 'filename': 'papers/paper_3.pdf', 'summary': 'We investigate the potential of large language models (LLMs) to serve as\\nefficient simulators for agentic search tasks in reinforcement learning (RL),\\nthereby reducing dependence on costly interaction'}, {'paper_title': 'From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms', 'filename': 'papers/paper_4.pdf', 'summary': 'Recent advancements in machine learning have spurred growing interests in\\nautomated interpreting quality assessment. Nevertheless, existing research\\nsuffers from insufficient examination of language u'}, {'paper_title': 'Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning', 'filename': 'papers/paper_5.pdf', 'summary': 'Amidst a shortage of qualified mental health professionals, the integration\\nof large language models (LLMs) into psychological applications offers a\\npromising way to alleviate the growing burden of me'}, {'paper_title': 'Reinforced Language Models for Sequential Decision Making', 'filename': 'papers/paper_6.pdf', 'summary': 'Large Language Models (LLMs) show potential as sequential decision-making\\nagents, but their application is often limited due to a reliance on large,\\ncomputationally expensive models. This creates a ne'}, {'paper_title': 'Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions', 'filename': 'papers/paper_7.pdf', 'summary': 'Memory is fundamental to intelligence, enabling learning, reasoning, and\\nadaptability across biological and artificial systems. While Transformer\\narchitectures excel at sequence modeling, they face cr'}, {'paper_title': 'Beyond \"Not Novel Enough\": Enriching Scholarly Critique with LLM-Assisted Feedback', 'filename': 'papers/paper_8.pdf', 'summary': 'Novelty assessment is a central yet understudied aspect of peer review,\\nparticularly in high volume fields like NLP where reviewer capacity is\\nincreasingly strained. We present a structured approach f'}, {'paper_title': 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models', 'filename': 'papers/paper_9.pdf', 'summary': 'Reinforcement learning with verifiable rewards (RLVR), which typically adopts\\nPass@1 as the reward, has faced the issues in balancing exploration and\\nexploitation, causing policies to prefer conservat'}, {'paper_title': 'Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs', 'filename': 'papers/paper_10.pdf', 'summary': 'Despite large language models (LLMs) have achieved remarkable success, their\\nprefix-only prompting paradigm and sequential generation process offer limited\\nflexibility for bidirectional information. D'}, {'paper_title': 'Learning from Natural Language Feedback for Personalized Question Answering', 'filename': 'papers/paper_11.pdf', 'summary': 'Personalization is crucial for enhancing both the effectiveness and user\\nsatisfaction of language technologies, particularly in information-seeking\\ntasks like question answering. Current approaches fo'}, {'paper_title': 'Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph', 'filename': 'papers/paper_12.pdf', 'summary': 'Millions of individuals worldwide are affected by deafness and hearing\\nimpairment. Sign language serves as a sophisticated means of communication for\\nthe deaf and hard of hearing. However, in societie'}, {'paper_title': 'Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages', 'filename': 'papers/paper_13.pdf', 'summary': 'This paper presents the first systematic study of strategies for translating\\nCoptic into French. Our comprehensive pipeline systematically evaluates: pivot\\nversus direct translation, the impact of pre'}, {'paper_title': 'eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM', 'filename': 'papers/paper_14.pdf', 'summary': 'This paper presents a feasibility study on the deployment of a European Deep\\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\\nmechanistic interpretability research on lar'}, {'paper_title': 'When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models', 'filename': 'papers/paper_15.pdf', 'summary': 'Multimodal Large Language Models (MLLMs) have demonstrated remarkable\\ncapabilities across a diverse range of multimodal tasks. However, these models\\nsuffer from a core problem known as text dominance:'}, {'paper_title': 'Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards', 'filename': 'papers/paper_16.pdf', 'summary': 'Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a\\nsignificant challenge, while existing outcome-based reward shaping struggles to\\ndefine meaningful immediate rewards without '}, {'paper_title': 'Improving Value-based Process Verifier via Low-Cost Variance Reduction', 'filename': 'papers/paper_17.pdf', 'summary': 'Large language models (LLMs) have achieved remarkable success in a wide range\\nof tasks. However, their reasoning capabilities, particularly in complex\\ndomains like mathematics, remain a significant ch'}, {'paper_title': 'Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment', 'filename': 'papers/paper_18.pdf', 'summary': 'The alignment of language models (LMs) with human preferences is critical for\\nbuilding reliable AI systems. The problem is typically framed as optimizing an\\nLM policy to maximize the expected reward t'}, {'paper_title': 'Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model', 'filename': 'papers/paper_19.pdf', 'summary': 'Full-process clinical diagnosis in the real world encompasses the entire\\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\\nartificial intelligence (AI), particularly large '}, {'paper_title': 'When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing', 'filename': 'papers/paper_20.pdf', 'summary': 'In the study of trustworthy Natural Language Processing (NLP), a number of\\nimportant research fields have emerged, including that of\\n\\\\textit{explainability} and \\\\textit{privacy}. While research intere'}]\n"
     ]
    }
   ],
   "source": [
    "def data_clection(max_results=50, categories=\"cs.CL\", save_paperlist=True, paperlist_filename=\"paperlist.json\"):\n",
    "\n",
    "    query = f\"cat:{categories}\"\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    results = search.results()\n",
    "\n",
    "    if not results:\n",
    "        print(\"No results found.\")  \n",
    "        return None\n",
    "    else:    \n",
    "        #print(f\"Found {len(list(results))} results.\")\n",
    "\n",
    "        # Create a dir to save PDFs if it doesn't exist\n",
    "        os.makedirs(\"papers\", exist_ok=True)\n",
    "\n",
    "        paperlist = []\n",
    "        print(f\"Starting download of {max_results} papers in category {categories}...\")\n",
    "        for index, result in enumerate(results):\n",
    "            text = f\"{index+1}. Title: {result.title} downloading ...\"\n",
    "\n",
    "            result.download_pdf(dirpath=os.getcwd()+\"\\\\papers\", filename=f\"paper_{index}.pdf\")\n",
    "            paperlist.append({\n",
    "                \"paper_title\": result.title,\n",
    "                \"filename\": f\"papers/paper_{index+1}.pdf\",\n",
    "                \"summary\": result.summary[:200]\n",
    "            })\n",
    "            print(f\"{text} ... Done\")\n",
    "            index += 1\n",
    "\n",
    "        \n",
    "        if save_paperlist:\n",
    "            with open(paperlist_filename, 'w', encoding=\"utf-8\") as f:\n",
    "                json.dump(paperlist, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Paper list saved to {paperlist_filename}\")\n",
    "\n",
    "        print(\"Download completed!\")\n",
    "        return(paperlist)\n",
    "\n",
    "print(data_clection(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe05c6",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d160a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ragpipeline:\n",
    "    def __init__(self, paperlist_filename=\"paperlist.json\", model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.paperlist_filename = paperlist_filename\n",
    "        self.model_name = model_name\n",
    "        self.paperlist = self.load_paperlist()\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        self.index = None  # Initialize FAISS index\n",
    "        self.chunks = []  \n",
    "        self.chunk_metadata = []\n",
    "        self.total_chunks = 0\n",
    "        self.total_vectors = 0\n",
    "        self.total_indexed = 0\n",
    "        # --- File Paths for Saved Data ---\n",
    "        self.FAISS_INDEX_FILE = \"index.faiss\"\n",
    "        self.CHUNKS_FILE = \"chunks.pkl\"\n",
    "        self.METADATA_FILE = \"metadata.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "        # New: Check for and load existing data on startup\n",
    "        self.load_index()\n",
    "        if self.index is None:\n",
    "            print(\"No saved index found. The pipeline needs to be built first.\")\n",
    "        else:\n",
    "            print(\"FAISS index and data loaded successfully.\")\n",
    "\n",
    "\n",
    "    def load_paperlist(self):\n",
    "        if os.path.exists(self.paperlist_filename):\n",
    "            with open(self.paperlist_filename, 'r', encoding=\"utf-8\") as f:\n",
    "                paperlist = json.load(f)\n",
    "            return paperlist\n",
    "        else:\n",
    "            print(f\"Paper list file {self.paperlist_filename} not found.\")\n",
    "            return []\n",
    "    \n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path:str) -> str:\n",
    "        \"\"\" Text Extraction: Extract raw text from each PDF. Clean and concatenate the page text into full-document strings\"\"\"\n",
    "        doc = None\n",
    "        try:\n",
    "            #Opening a document\n",
    "            doc = pymupdf.open(pdf_path)\n",
    "            pages=[]\n",
    "            for page in doc:\n",
    "                page_text = page.get_text().strip() # Get raw text from the page\n",
    "                pages.append(page_text)\n",
    "            full_text = \"\\n\".join(pages)  # Concatenate all page texts into a single string\n",
    "            return full_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_path}: {e}\")\n",
    "        finally:\n",
    "            if doc:\n",
    "                doc.close()\n",
    "\n",
    "\n",
    "    def chunk_text_sliding_window(self, text:str, max_token:int =512, overlap: int =50) -> list[str]:\n",
    "        \"\"\"Chunking Logic (Sliding Windows)\"\"\"\n",
    "        tokens = text.split()  # Simple tokenization by whitespace\n",
    "        chunks = []\n",
    "        step = max_token - overlap\n",
    "        for i in range(0, len(tokens), step):\n",
    "            chunk = tokens[i:i + max_token]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            if i + max_token >= len(tokens):\n",
    "                break\n",
    "        return chunks\n",
    "    \n",
    "\n",
    "    def embadding_text_chunks(self, chunks: list[str]) -> list[tuple[str, list[float]]]:\n",
    "        \"\"\"Embedding Logic: Convert text chunks into embeddings using a pre-trained model.\n",
    "           Sample return format: [(\"This is chunk 1\", embedding1), (\"This is chunk 2\", embedding2)]\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            print(\"No text chunks to embed.\")\n",
    "            return []\n",
    "        embeddings = self.model.encode(chunks, show_progress_bar=True) # GPU and pyTorch: convert_to_tensor=True  \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def build_index(self, embeddings: np.ndarray) -> list[str]:\n",
    "        \"\"\"Build FAISS Index\n",
    "            embeddings should be a 2D numpy array of shape (num_chunks, dimension), example: (100, 384)\n",
    "        \"\"\"\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)  # FAISS needs to know the dimensionality of the vectors it will be indexing. L2 distance (Euclidean distance)\n",
    "        self.index.add(embeddings.astype(np.float32))  # Ensure embeddings are in float32 format\n",
    "        # print(f\"FAISS index built with {embeddings.shape[0]} vectors of dimension {dim}.\")\n",
    "        # print(f\"Number of vectors in index: {self.index.ntotal}\")\n",
    "\n",
    "    def save_index(self,index_path=\"faiss_data\"):\n",
    "        \"\"\"\n",
    "        Saves the FAISS index, chunks, and metadata to disk.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.index is None:\n",
    "            print(\"No index to save.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(index_path, exist_ok=True)\n",
    "            \n",
    "            # Define file paths\n",
    "            faiss_index_file = os.path.join(index_path, self.FAISS_INDEX_FILE)\n",
    "            chunks_file = os.path.join(index_path, self.CHUNKS_FILE)\n",
    "            metadata_file = os.path.join(index_path, self.METADATA_FILE)\n",
    "\n",
    "            # Save the FAISS index\n",
    "            faiss.write_index(self.index, faiss_index_file)\n",
    "            print(f\"\\nFAISS index saved to {faiss_index_file}\")\n",
    "\n",
    "            # Save the chunks list\n",
    "            with open(chunks_file, 'wb') as f:\n",
    "                pickle.dump(self.chunks, f)\n",
    "            print(f\"Chunks saved to {chunks_file}\")\n",
    "\n",
    "            # Save the chunk metadata\n",
    "            with open(metadata_file, 'wb') as f:\n",
    "                pickle.dump(self.chunk_metadata, f)\n",
    "            print(f\"Metadata saved to {metadata_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving files: {e}\")\n",
    "\n",
    "        \n",
    "    def load_index(self, index_path=\"faiss_data\") -> bool:\n",
    "        \"\"\"\n",
    "        Loads the FAISS index, chunks, and metadata from disk if they exist.\n",
    "        \"\"\"\n",
    "        FAISS_INDEX_FILE = os.path.join(index_path, self.FAISS_INDEX_FILE)\n",
    "        CHUNKS_FILE = os.path.join(index_path, self.CHUNKS_FILE)\n",
    "        METADATA_FILE = os.path.join(index_path, self.METADATA_FILE)\n",
    "        \n",
    "        if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(CHUNKS_FILE) and os.path.exists(METADATA_FILE):\n",
    "            try:\n",
    "                # Load the FAISS index\n",
    "                self.index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "                \n",
    "                # Load the chunks list\n",
    "                with open(CHUNKS_FILE, 'rb') as f:\n",
    "                    self.chunks = pickle.load(f)\n",
    "\n",
    "                # Load the chunk metadata\n",
    "                with open(METADATA_FILE, 'rb') as f:\n",
    "                    self.chunk_metadata = pickle.load(f)\n",
    "                \n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading saved files: {e}\")\n",
    "                self.index = None\n",
    "                self.chunks = []\n",
    "                self.chunk_metadata = []\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Search the FAISS index for the k nearest neighbors of the query embedding.\n",
    "           Returns distances and indices of the nearest neighbors.\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"FAISS index is not built. Call build_faiss_index() first.\")\n",
    "        \n",
    "        # Get query_embedding from the text\n",
    "        query_embedding = self.model.encode([query], show_progress_bar=False)  # Encode the query\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        if query_embedding.shape[0] != 1:\n",
    "            raise ValueError(\"Query embedding should be a single vector, but got shape: {}\".format(query_embedding.shape))\n",
    "        if query_embedding.shape[1] != self.index.d:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} does not match index dimension {self.index.d}.\")\n",
    "        \n",
    "        query_embedding = query_embedding.astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, k)  # Search the index\n",
    "        print(f\"Search completed. Found {len(distances[0])} nearest neighbors.\")\n",
    "\n",
    "        result=[]\n",
    "        for indice, distance in zip(indices[0], distances[0]):\n",
    "            result.append({\n",
    "                \"distance\": float(distance),\n",
    "                \"chunk\": self.chunks[indice],\n",
    "                \"metadata\":self.chunk_metadata[indice]\n",
    "            })\n",
    "\n",
    "        return result\n",
    "        \n",
    "\n",
    "    def build_rag_runner(self):\n",
    "        \"\"\" Process papers \"\"\"\n",
    "       \n",
    "        if not self.paperlist:\n",
    "            print(\"No papers found in the paper list. Please run data clection first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Building RAG from {len(self.paperlist)} PDF files ...\")\n",
    "\n",
    "        idx =1\n",
    "        all_embeddings = []\n",
    "        for paper in tqdm(self.paperlist, desc=\"Processing papers\"):\n",
    "\n",
    "            text = self.extract_text_from_pdf(paper['filename'])\n",
    "            if text:\n",
    "                chunks = self.chunk_text_sliding_window(text)\n",
    "                embeddings = self.embadding_text_chunks(chunks)\n",
    "                \n",
    "                # Store chunks and metadata, and append embeddings to a single list\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    self.chunks.append(chunk)\n",
    "                    self.chunk_metadata.append({\n",
    "                        \"paper_title\": paper['paper_title'],\n",
    "                        \"filename\": paper['filename'],\n",
    "                        \"chunk_index_in_paper\": i,\n",
    "                    })\n",
    "                all_embeddings.append(embeddings)\n",
    "                # For statistics\n",
    "                self.total_chunks += len(chunks)\n",
    "                self.total_vectors += len(embeddings)\n",
    "               \n",
    "                idx += 1  \n",
    "\n",
    "        if all_embeddings:\n",
    "            final_embeddings =np.concatenate(all_embeddings, axis=0)  # Concatenate all embeddings into a single array\n",
    "\n",
    "            # Build the FAISS index with the final embeddings\n",
    "            self.build_index(final_embeddings)\n",
    "\n",
    "            # Finally, save the built index and collected data\n",
    "            self.save_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f6645",
   "metadata": {},
   "source": [
    "## Build RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c945152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Pipeline Starting ...\n",
      "No saved index found. The pipeline needs to be built first.\n",
      "Building RAG from 20 PDF files ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]  1.46s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]  1.43s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]  1.10s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]  1.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]  1.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]  1.10it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]  1.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]  1.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]  1.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s],  1.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s],  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.49it/s],  1.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s],  1.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s],  1.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s],  1.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s],  1.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s],  1.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s],  1.13s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s],  1.05s/it]\n",
      "Processing papers: 100%|██████████| 20/20 [00:18<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to faiss_data\\index.faiss\n",
      "Chunks saved to faiss_data\\chunks.pkl\n",
      "Metadata saved to faiss_data\\metadata.pkl\n",
      "\n",
      "Total papers processed:  20\n",
      "Total text chunks created:  552\n",
      "Total embeddings generated:  552\n",
      "\n",
      "RAG Pipeline Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRAG Pipeline Starting ...\")\n",
    "rag = ragpipeline(paperlist_filename=\"paperlist.json\", model_name=\"all-MiniLM-L6-v2\")\n",
    "rag.build_rag_runner()\n",
    "\n",
    "#Statistics\n",
    "print(\"\\nTotal papers processed: \", len(rag.paperlist))\n",
    "print(\"Total text chunks created: \", rag.total_chunks)\n",
    "print(\"Total embeddings generated: \", rag.total_vectors)\n",
    "#print(\"Total vectors indexed in FAISS: \", rag.total_indexed)\n",
    "\n",
    "print(\"\\nRAG Pipeline Completed.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c4ca1",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Search through the saved faiss index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd5baab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and data loaded successfully.\n",
      "Search completed. Found 3 nearest neighbors.\n",
      "\n",
      "Question: What is BERT and how does it work?\n",
      "\n",
      "Result: 1\n",
      "Title: Searching for Privacy Risks in LLM Agents via Simulation\n",
      "Filename: papers/paper_1.pdf\n",
      "Distance: 1.0834\n",
      "Chunk Choosed: volume 1 (long and short papers), 2019, pp. 4171–4186. [131] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A ro- bustly optimized be...\n",
      "\n",
      "Result: 2\n",
      "Title: Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph\n",
      "Filename: papers/paper_12.pdf\n",
      "Distance: 1.1304\n",
      "Chunk Choosed: that fine-tunes BERT-based models to predict human judgments of text quality. It combines semantic similarity, flu- ency, and grammaticality into a single score, and has been widely used to evaluate m...\n",
      "\n",
      "Result: 3\n",
      "Title: Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models\n",
      "Filename: papers/paper_9.pdf\n",
      "Distance: 1.2558\n",
      "Chunk Choosed: is to minimize the negative evidence lower bound (NELBO), an upper bound on the negative log-likelihood of the data. For masked dLLMs, the NELBO reduces to a weighted log- likelihood loss, with weight...\n"
     ]
    }
   ],
   "source": [
    "rag = ragpipeline(paperlist_filename=\"paperlist.json\", model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"What is BERT and how does it work?\"\n",
    "#query =\"What are transformers in NLP?\"\n",
    "results = rag.search(query, k=3)\n",
    "\n",
    "print(\"\\nQuestion:\", query)\n",
    "for idx, result in enumerate(results):\n",
    "    #print(result)\n",
    "    print(f\"\\nResult: {idx + 1}\")\n",
    "    print(f\"Title: {result['metadata']['paper_title']}\")\n",
    "    print(f\"Filename: {result['metadata']['filename']}\")\n",
    "    print(f\"Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Chunk Choosed: {result['chunk'][:200]}...\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
