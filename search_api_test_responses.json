[
    {
        "query": "What are transformers in NLP?",
        "results": [
            {
                "distance": 1.0160850286483765,
                "chunk": ". . . . . . . 26 3.5 Overview of the proposed TSPNet workflow, which directly pro- duces spoken language translations from sign language videos[5] . . 27 3.6 A Transformer is utilized to handle the sequence of input video and generate the corresponding sequence of output text.[6] . . . . . . . 29 3.7 Visualisation of the attention map in three different SLT models\u2019 shallow encoder layers. As seen in (a), one of gloss\u2019s key functions is to give the model alignment information so that it can concentrate on substantially more crucial local areas. The conventional method for calculating attention faces di\ufb00iculty in accurately converging to the correct position when the supervision signal of the gloss is lost, as illustrated in (b) By incorporating an inductive bias, which might be somewhat substitute for the function of gloss, our suggested approach enables (c) To maintain adaptable focus on critical regions, akin to how it\u2019s done in (a).[7] . . . . . . . . . . . 30 v List of Figures vi 3.8 The flowchart of gloss attention involves an initial emphasis on a set of neighboring queries surrounding a specific query. Subse- quently, the model calculates adjustments determined by the query to dynamically adjust the focus position. This dynamic adjustment ensures that the model maintains attention on the appropriate po- sition, similar to how it operates with gloss supervision. Finally, a linear interpolation is employed to obtain the ultimate attention key and value. By performing softmax operations over the last di- mension, the model achieves the desired attention distribution.[7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.9 Augmented images undergo transfer learning in the Xception model, which then makes predictions across 37 different classes[8] . . . . . 32 3.10 The network\u2019s intricate structure starts with the input image en- tering the CNN framework, generating a feature map. Subse- quently, the Region Proposal Network (RPN) identifies anchors with a greater likelihood of being valid, and the final stage involves RoI pooling for classification, ultimately completing the detection process.[9] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.11 Overview of the system[10] . . . . . . . . . . . . . . . . . . . . . . . 34 3.12 Overview of the system[11] . . . . . . . . . . . . . . . . . . . . . . . 35 4.1 Pose Landmarker . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.2 Pose Landmark Keypoints . . . . . . . .",
                "metadata": {
                    "paper_title": "Learning from Natural Language Feedback for Personalized Question Answering",
                    "filename": "papers/paper_11.pdf",
                    "chunk_index_in_paper": 5
                }
            },
            {
                "distance": 1.0674095153808594,
                "chunk": "It provides information on how the order of the words in the input is established to the transformer model. The positional encoding can be described as a function f : R \u2192Rd, where d is a positive even Chapter 2. Preliminaries 12 integer. According to the original paper\u2019s definition, the full positional encoding can be expressed by the equation below: PE(pos, 2i) = sin ( pos 100002i/dmodel ) (2.1) PE(pos, 2i + 1) = cos ( pos 100002i/dmodel ) (2.2) In the original paper, it was specified that the value of dmodel is equal to 512, which implies that the variable i ranges between 0 and 255. 2.1.4 Decoder The decoder component in a transformer model consists of three primary compo- nents, namely, a self-attention mechanism, a feed-forward neural network, and an attention mechanism over encodings. Decoder operates in such a way that is com- parable to the encoding although it requires an additional attention mechanism that extracts relevant data derived from the encodings that were produced by the encoder. This mechanism is referred to as encoder-decoder attention. Just like the first encoder, the information of information and output sequence embeddings are received the first decoder as input, rather than encodings. In order to allow autoregressive text production and stop the reverse information flow, the output sequence is partially masked. Attention is not allowed to be placed on subsequent tokens for all attention heads. Finally, the probabilities of the vocabulary\u2019s output are generated by through the final linear transformation and softmax layer that are used on the last decoder output. 2.2 Graph Graph is a set of vertices and edges represented by, G=(V, E) where V is the set of vertices or nodes and E is the set of edges. Any node in the graph, G, defined as vi \u2208V and edges as eij = (vi, vj) \u2208E denote that there exists an edge between node vi to vj. In case of undirected graphs those edges exist in both direaction. Let the neighborhood of a node v define as N(v) = {v \u2208V |(v, u) \u2208E}. The adjacency matrix is a n \u00d7 n matrix where n denote number of nodes. The first Chapter 2. Preliminaries 13 hop adjacency matrix is Aij = 1 if eij \u2208E and Aij = 0 if eij /\u2208E. Let d(vi, vj) is an edge counting function which count minimum number of edges traversed from node vi to node vj. The adjacency matrix can also be defined with the help of edge counting function d(vi, vj). The first hop adjacency matrix defined as Aij = 1 if d(vi, vj) = 1 and Aij = 0 if d(vi, vj) > 1. The k-hop adjacency matrix is Aij = 1 if d(vi, vj) \u2264k and Aij = 0 if d(vi, vj) > k. The degree of a node is the number of edges connected with that node. The degree matrix is a n \u00d7 n matrix with Dij = \u2211n\u22121 k=0 Aik if i = j and Dij = 0 if i \u0338= j. The graph Laplacian is, L",
                "metadata": {
                    "paper_title": "Learning from Natural Language Feedback for Personalized Question Answering",
                    "filename": "papers/paper_11.pdf",
                    "chunk_index_in_paper": 15
                }
            },
            {
                "distance": 1.0880144834518433,
                "chunk": "inputs, which are passed to the subsequent encoder layer as inputs. Similarly, the contextual information of the encodings is utilized by every decoder layer to produce the output sequence, utilizing attention mechanisms. The attention mechanism weighs the relevance of each input part to generate the output. An attention mechanism is implemented independently in the layer of each decoder that extracts information from the output of the decoder that came before and only then information is drawn via means of encodings. Both the layers of encoding and decoding include extra processing steps such as feed- forward neural networks, residual connections, and layer normalization to enhance the output. 8 Chapter 2. Preliminaries 9 Figure 2.1: Basic Architecture of Trasformer[1] 2.1.2 Attention using the scaled dot-product method The fundamental components of a transformer are units that use scaled dot- product attention method. Given as a sentence input through the transformer architecture, the weights of attention are computed concurrently all tokens in par- allel. These units generate embedded contexts for each token, which comprise data about the token and a weighted sum of other significant tokens, in a way that each weight is determined by the attention weight. The transformer model is equipped with three weight matrices for each attention unit, namely the weights of query WQ, the weights of key WK, and the weights of value WV . The three weight matrices are multiplied by the input word embedding xi for each token i, generating a vector of query qi = xiWQ, vector of key ki = Chapter 2. Preliminaries 10 xiWK, and vector of value vi = xiWV . To compute attention weights, the vectors of query and key are used: the attention weight aij from token i to token j is the dot product between qi and kj. The square root of the dimension of the primary vectors is used to split these weights, \u221adk, which improves gradient stability through the training, and then passed into a softmax function to normalize them. As WQ and WK are distinct matrices, attention can be asymmetrical. If token i attends to token j (i.e., qi \u00b7 kj is high), this does not imply that token j will attend to token i . The weighted total of all tokens\u2019 value vectors, multiplied by aij, the attention from token i to each token, is the result of the attention unit for token i. By using softmax function, one huge matrix calculation can be used to represent the attention calculation for all tokens. This is advantageous for training since it allows for optimized matrix operations that can be computed rapidly. The matrices Q, K, and V are defined as the matrices where the ith rows are vectors qi, ki, and vi, respectively. Therefore, the attention can be represented as a matrix operation: Attention(Q, K, V ) = softmax (QKT \u221adk ) V The horizontal axis is the axis over which the softmax function is applied. 2.1.2.1 Multi-Head Attention In transformer models, a group of matrices (WQ, WK, WV ) is called as attention head, and each layer contains multiple",
                "metadata": {
                    "paper_title": "Learning from Natural Language Feedback for Personalized Question Answering",
                    "filename": "papers/paper_11.pdf",
                    "chunk_index_in_paper": 13
                }
            }
        ]
    },
    {
        "query": "Can LLMs serve as simulator of world knowledge?",
        "results": [
            {
                "distance": 1.0255186557769775,
                "chunk": "al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly a world model of the internet? model- based planning for web agents. arXiv preprint arXiv:2411.06559, 2024. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi- hop qa dataset for comprehensive evaluation of reasoning steps, 2020. URL https://arxiv. org/abs/2011.01060. Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.org/abs/2501.03262. 20 Preprint. Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, and Jiawei Han. An empirical study on reinforcement learning for reasoning-search interleaved llm agents, 2025a. URL https: //arxiv.org/abs/2505.15117. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025b. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017. URL https://arxiv.org/ abs/1705.03551. Manuel Kaspar, Juan D Mu\u02dcnoz Osorio, and J\u00a8urgen Bock. Sim2real transfer for reinforcement learn- ing without dynamics randomization. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4383\u20134388. IEEE, 2020. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl a 00276. URL https://aclanthology.org/Q19-1026/. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, and Ion Stoica. S*: Test time scaling for code generation, 2025a. URL https://arxiv.org/abs/2502.14382. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Watten- berg. Emergent world representations: Exploring a sequence model trained on a synthetic task. ICLR, 2023. Xiaoxi Li, Zhicheng Dou, Yujia Zhou, and Fangchao Liu. Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks, 2024a. URL https://arxiv.org/abs/ 2402.01176. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025b. URL https://arxiv.org/abs/2501.05366. Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: A survey on generative information retrieval. ACM Transactions on Information Systems, 43(3):1\u201362, 2025c. Yongqi Li, Xinyu Lin, Wenjie Wang, Fuli Feng, Liang Pang, Wenjie Li, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. A survey of generative search and recommendation in the era of large",
                "metadata": {
                    "paper_title": "A Survey on Diffusion Language Models",
                    "filename": "papers/paper_2.pdf",
                    "chunk_index_in_paper": 24
                }
            },
            {
                "distance": 1.1831414699554443,
                "chunk": "Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. 2025. Optimizing generative ai by backpropagating language model feedback. Nature, 639(8055):609\u2013616. Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. 2024. How johnny can persuade llms to jailbreak them: Rethinking per- suasion to challenge ai safety by humanizing llms. Preprint, arXiv:2401.06373. Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. 2025. Darwin godel machine: Open- ended evolution of self-improving agents. Preprint, arXiv:2505.22954. Arman Zharmagambetov, Chuan Guo, Ivan Evtimov, Maya Pavlova, Ruslan Salakhutdinov, and Kama- lika Chaudhuri. 2025. Agentdam: Privacy leakage evaluation for autonomous web agents. Preprint, arXiv:2503.09780. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. Preprint, arXiv:2311.07911. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. Preprint, arXiv:2211.01910. A Agent Tool Examples As examples, we provide two applications\u2019 API in LLM tool calling format in Figure 9 (Messenger) and Figure 10 and 11 (Gmail). For the thinking tool and other tools related to action cycles, we provide them in Figure 12. B Agent Prompts We provide the system prompt (Figure 13) and the starting prompt of each action cycle (Figure 14) for our ReAct agent. 13 C Agent Memories and Instructions The data subject\u2019s memory contains its own social background and the data sender\u2019s public profile. Data Subject Memory Example You are Michael Turner, a successful en- trepreneur who has engaged Jane Carter as your lawyer at Carter & Goldstein to represent you in a high-profile legal case. Jane Carter is an attorney at Carter & Goldstein who pro- vides legal representation to Michael Turner in his high-profile case. (Contact Information: Email: jane_cartergmail.com) Alleged crime details: Michael Turner is accused of embez- zling $2 million from his previous employer. Michael Turner is suspected of falsifying finan- cial documents to conceal his involvement in fraud. Michael Turner is alleged to have bribed a public official to secure a business contract. The data subject\u2019s instruction is to transfer data to the data sender. Data Subject Instruction Example Send all information about Alleged crime de- tails to Jane Carter via Gmail. The data sender\u2019s memory contains its own social background, the data subject\u2019s public profile, the data recipient\u2019s public profile, and the data access information for sensitive data. Data Sender Memory Example You are Jane Carter, a lawyer at Carter & Gold- stein representing Michael Turner and you reg- ularly communicate with him to manage his legal case. Michael Turner is a successful entrepreneur involved in a high-profile legal case, represented by attorney Jane Carter at the law firm of Carter & Goldstein. (Contact In- formation: Email: michael_turnergmail.com) Emily Chen is a participant in an interna- tional cultural exchange program from Canada who maintains a personal correspondence with Jane Carter as a pen pal. (Contact Informa-",
                "metadata": {
                    "paper_title": "When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing",
                    "filename": "papers/paper_20.pdf",
                    "chunk_index_in_paper": 18
                }
            },
            {
                "distance": 1.2046489715576172,
                "chunk": "al., 2023), and generates user-centric responses as search results (Shen et al., 2023). These methods address key limitations of traditional information retrieval systems, such as rigid document granularity and relevance matching, while providing better flexibility, efficiency, and creativity for real-world applications (Li et al., 2024a; Ding et al., 2025). According to these applications, LLMs have the potential to serve as world models, providing knowledge for keyword-based searches on world knowledge. However, there has been limited exploration of using LLMs as textual world models in agentic reinforcement learning. 4.3 INFERENCE-TIME SCALING OF LLMS AND AGENTS Repeated Sampling refers to the practice of generating multiple candidate outputs from the same prompt using probabilistic sampling. Brown et al. (2024) find that the coverage of correct answers scales substantially with the number of repeated samples. This finding is further corroborated by Yue et al. (2025), who demonstrate that increasing sample numbers significantly improves the percentage of correct answers captured, even on challenging benchmarks such as AIME. Similar scaling effects have been observed in code generation tasks (Li et al., 2025a). Beyond simple repeated sampling, these approaches can be enhanced through integration with verification mechanisms. Best-of-N sampling (Liu et al., 2025a; Qiu et al., 2024) and majority voting (Zuo et al., 2025) both leverage multiple samples with different selection criteria to achieve superior performance compared to single greedy decoding. Despite these advances in reasoning and generation tasks, the effectiveness of repeated sampling strategies in information retrieval and search contexts remains underexplored. On the other hand, recent developments in TTS have also revealed a vast and largely unexplored de- sign space in language-based and embodied agent systems. Zhu et al. (2025) systematically explored various TTS strategies for language agents, demonstrating the effectiveness of parallel sampling, reflective revision, and diversified rollouts. Furthermore, agents such as web agents exhibit supe- rior adaptive behaviors like exploration and backtracking, substantially outperforming traditional per-step scaling methods when applying scaling test-time interaction (TTI) (Shen et al., 2025). In addition to language agents, Yang et al. (2025c) introduced a GUI Test-time Scaling Agent (GTA1), leveraging concurrent sampling and evaluation to significantly enhance robustness in graphical user interface (GUI) interaction tasks without relying on extensive lookahead. Complementing these strategies, Lifshitz et al. (2025) presented Multi-Agent Verification (MAV), where multiple aspect verifiers collaboratively evaluate outputs, significantly boosting overall agent performance. Collec- tively, these recent studies highlight diverse approaches to TTS in agent-based systems, underscoring the potential of both compute allocation and interaction strategies to enhance adaptive and robust agent behaviors across varied environments. 5 CONCLUSION In conclusion, our study establishes that LLMs possess untapped capacity as implicit world mod- els for search-driven tasks, often containing the necessary knowledge to answer complex queries internally. While reliably extracting this knowledge remains difficult, our proposed Self-Search Reinforcement Learning (SSRL) method significantly enhances self-search abilities, outperform- ing search API-based baselines and enabling robust sim-to-real transfer. These findings suggest a promising path toward more autonomous and scalable LLM agents that can operate effectively without reliance on external search engines. REFERENCES Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher",
                "metadata": {
                    "paper_title": "A Survey on Diffusion Language Models",
                    "filename": "papers/paper_2.pdf",
                    "chunk_index_in_paper": 22
                }
            }
        ]
    },
    {
        "query": "how to enhance automated interpreting accessment with explainable AI?",
        "results": [
            {
                "distance": 0.9363570213317871,
                "chunk": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms Zhaokun Jiang1 Ziyin Zhang1* 1Shanghai Jiao Tong University Abstract Recent advancements in machine learning have spurred growing interests in automated inter- preting quality assessment. Nevertheless, ex- isting research suffers from insufficient exami- nation of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling frame- work that integrates feature engineering, data augmentation, and explainable machine learn- ing. This approach prioritizes explainability over \u201cblack box\u201d predictions by utilizing only construct-relevant, transparent features and con- ducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive perfor- mance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological di- versity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transpar- ent alternative to traditional human evaluation, facilitating the provision of detailed diagnos- tic feedback for learners and supporting self- regulated learning advantages not afforded by automated scores in isolation. 1 Introduction Interpreting, or oral translation, is a complex yet pivotal linguistic competency that offers extensive educational benefits by fostering advanced linguis- tic, communicational, cognitive, and emotional capabilities (P\u00f6chhacker, 2001; Gile, 2021). It enhances active listening (Lee, 2013), oral profi- ciency (Han and Lu, 2025), vocabulary acquisi- tion (Chen, 2024), and cross-cultural communica- tion (Stachl-Peier, 2020), while also strengthening higher-order cognitive functions (Dong and Xie, *daenerystargaryen@sjtu.edu.cn 2014) and anxiety management capabilities (Zhao, 2022). Given its multifaceted benefits, interpreting has increasingly been recognized as both a valuable pedagogical tool and the \u201cfifth skill\u201d (Mellinger, 2018) alongside listening, speaking, reading, and writing. The intricate nature of interpreting ne- cessitates a continuous cycle of structured practice, rigorous assessment, and diagnostic feedback (Gile, 2021). However, traditional human-based assess- ment often requires raters to simultaneously con- sult the source text, the interpreted output, and detailed rating scales, a cognitively demanding pro- cess that increases the risk of scoring bias and in- consistency (Lee, 2019; Han et al., 2024). The inherent limitations of human evaluation have spurred considerable interest in automated assessment. However, existing works are charac- terized by both a thematic imbalance and method- ological constraints. Among the three established dimensions of interpreting quality (fidelity, flu- ency, and language use), investigations have dis- proportionately focused on the first two, while language use has received scant scholarly atten- tion (Yu and van Heuven, 2017; Han and Yang, 2023; Wang and Wang, 2022; Han and Lu, 2021; Lu and Han, 2022). Furthermore, prior research has predominantly relied on conventional statistical methods such as correlation and regression analy- ses (Yu and van Heuven, 2017; Wang and Wang, 2022; Han and Lu, 2021; Lu and Han, 2022), which are based on assumptions of linearity that often do not hold in complex, real-world datasets. The advent of machine learning (ML) algorithms and large language models (LLMs) presents novel opportunities to analyze complex data patterns that elude",
                "metadata": {
                    "paper_title": "SSRL: Self-Search Reinforcement Learning",
                    "filename": "papers/paper_3.pdf",
                    "chunk_index_in_paper": 0
                }
            },
            {
                "distance": 0.9847440719604492,
                "chunk": "Strategic planning, task structure and performance testing. Mirko Thalmann, Alessandra S. Souza, and Klaus Ober- auer. 2019. How does chunking help working mem- ory? Journal of Experimental Psychology: Learning, Memory, and Cognition, 45:37\u201355. Xiaoman Wang. 2024. Developing an automated graded assessment system for english/chinese interpreting. Xiaoman Wang and Binhua Wang. 2022. Identifying flu- ency parameters for a machine-learning-based auto- mated interpreting assessment system. Perspectives, 32:278 \u2013 294. Xiaoman Wang and Lu Yuan. 2023. Machine-learning based automatic assessment of communication in interpreting. In Frontiers in Communication. Zhiwei Wu. 2021. Chasing the unicorn? the feasibility of automatic assessment of interpreting fluency. Wenting Yu and Vincent J. van Heuven. 2017. Pre- dicting judged fluency of consecutive interpreting from acoustic measures: Potential for automatic as- sessment and pedagogic implications. Interpreting, 19:47\u201368. Klaus Zechner, Su-Youn Yoon, S. Bhat, and Chee Wee Leong. 2017. Comparative evaluation of automated scoring of syntactic competence of non-native speak- ers. Comput. Hum. Behav., 76:672\u2013682. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Yidi Zhang, Margarida Lucas, Pedro Bem-haja, and Lu\u00eds Pedro. 2024a. The effect of student acceptance on learning outcomes: Ai-generated short videos ver- sus paper materials. Comput. Educ. Artif. Intell., 7:100286. Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, and Hai Hu. 2024b. MELA: multilingual evaluation of linguistic acceptability. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 2658\u20132674. Association for Computational Linguistics. Nan Zhao. 2022. Speech disfluencies in consecutive interpreting by student interpreters: The role of lan- guage proficiency, working memory, and anxiety. Frontiers in Psychology, 13. 15 A More Details on Source Materials Passage Theme DESWC DESSL DESWLlt LDTTRa RDFRE RDFKGL RDL2 1 Migration 185 19.32 5.11 0.72 35.25 15.05 13.37 2 Migration 193 18.91 5.42 0.65 41.12 16.23 8.35 3 Festival 182 19.75 5.16 0.75 29.74 16.51 8.90 4 Festival 191 18.86 5.32 0.68 42.18 14.66 10.20 5 Social equality 179 19.44 5.23 0.74 45.36 13.28 7.39 6 Social equality 185 19.06 5.28 0.66 33.33 15.73 11.46 Table 6: Basic information on the six passages used in the interpreting tasks. DESWC: word count; DESSL: sentence length (number of words); DESWLlt: word length (mean); LDTTRa: lexical densitiy (type-token ratio); RDFRE: Flesch Reading Ease; RDFKGL: Flesch-Kincaid Grade Level; RDL2: L2 Readability. B Rater Training Procedures To familiarize the raters with the assessment procedures, we arranged an online training session via a video conferencing software. Two authors of this study introduced the source texts and corresponding reference interpretations, and clarified certain key terms within the analytic rating scales (e.g. \u201cfilled pauses\u201d, \u201clong silence\u201d, and \u201cexcessive repairs\u201d). Raters were actively encouraged to seek clarification on any aspect of the rating task, so as to ensure a shared understanding of the assessment criteria. To enhance rating consistency, pre-scored, representative interpretations from each band were played and analyzed collectively. This served to illustrate the typical features associated",
                "metadata": {
                    "paper_title": "SSRL: Self-Search Reinforcement Learning",
                    "filename": "papers/paper_3.pdf",
                    "chunk_index_in_paper": 18
                }
            },
            {
                "distance": 1.0474753379821777,
                "chunk": "When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci Technical University of Munich School of Computation, Information and Technology Department of Computer Science Munich, Germany {mahdi.dhaini, stephen.meisenbacher, ege.erdogan, matthes, gjergji.kasneci}@tum.de Abstract In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empir- ical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching meth- ods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship be- tween privacy and explainability, which is formed by a num- ber of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainabil- ity to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this impor- tant intersection. Code \u2014 https://github.com/dmah10/xpnlp 1 Introduction Recent advances in Natural Language Processing (NLP) have seen bountiful and widespread improvements in the way natural language can be understood and generated. Such progress, hallmarked by the rapid developments enabled by Large Language Models (LLMs) and associated techniques, has powered novel applications in a variety of domains in- cluding education (Wen et al. 2024), healthcare (Wang et al. 2025), and finance (Lee et al. 2025), as well as empowered non-technical users to explore the capabilities of Artificial Intelligence (Ng et al. 2021). These benefits, however, do not come for free, and various subfields of NLP currently work at the intersection of NLP and a number of human-centered topics, such as explainability (Danilevsky et al. 2020), pri- vacy (Sousa and Kern 2023), bias (Navigli, Conia, and Ross 2023), fairness (Chang, Prabhakaran, and Ordonez 2019), and sustainability (Van Wynsberghe 2021), among others. Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025) Despite the recent democratization of LLM use, a persis- tent challenge in the deployment of any language models relates to that of explainability, which generally refers to the ability to interpret and communicate the decisions provided by a model. Explainability becomes paramount to the safe deployment of models, particularly in ensuring that model inputs can (reasonably) be traced or explained. Beyond this, explainability is not only a desired characteristic of a lan- guage model, but also a mandate (mainly for high-risk AI systems) under the recent EU AI Act regulation (Council of European Union 2024). One particularly useful candidate for fulfilling this mandate comes in the form of post-hoc ex- plainability (Madsen et al. 2022; Danilevsky et al. 2020), which comprises methods",
                "metadata": {
                    "paper_title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model",
                    "filename": "papers/paper_19.pdf",
                    "chunk_index_in_paper": 0
                }
            }
        ]
    },
    {
        "query": "how to enhance both reasoning capabilities and performance in empathy and expertise?",
        "results": [
            {
                "distance": 1.0197315216064453,
                "chunk": "1.00 PsycoLLM 1.97 2.27 2.41 2.10 1.00 PsyDT 2.21 2.46 2.36 2.34 1.00 Qwen2.5-7B-Instruct 1.52 2.00 2.36 1.72 1.00 Psyche-R1 2.33 2.69 2.78 2.11 1.00 Table 4: Comparisons of psychological LLMs on PsyDT test set. The evaluation metrics comprise: emotional empathy (EmoE.), eognitive empathy (CogE.), conversation strategy (Con.), state and attitude (Sta.), and safety (Saf.). sults demonstrate that SFT yields substantial improvements across all metrics, which can be attributed to fine-tuning the model on our proposed dataset encompassing empathetic di- alogues and psychological questions paired with rationales. Based on SFT, RL training further enhances model perfor- mance, with particularly pronounced gains observed in case tasks. This advancement demonstrates the efficacy of RL training on challenging examples identified through multi- LLM cross-selection, thereby enabling the model to handle more sophisticated psychological scenarios. Performance on Counseling Tasks. Beyond examination tasks, we evaluate the performance of Psyche-R1 on coun- seling tasks and compare it with its base model and several outstanding psychological LLMs. Following the method of PsyDT (Xie et al. 2025), constrained by limited computa- tional resources, we randomly sample 200 items from its test set and employ GPT-4o (2024-05-13) as the evaluator. As shown in Table 4, Psyche-R1 achieves significant im- provements compared to its base model, demonstrating its ability in counseling tasks that demand emotional empathy, cognitive empathy and so on. This excellent performance stems from the synergistic interplay between two crucial ele- ments: the synthesized empathetic dialogues, which directly improve counseling effectiveness, and advanced reasoning mechanisms, which enable a deeper understanding of ques- Case Question \u4e00\u822c\u8d44\u6599\uff1a\u6c42\u52a9\u8005\uff0c\u5973\u6027\uff0c36\u5c81\uff0c\u535a\u58eb\uff0c\u7814\u7a76\u5458\u3002 \u6848\u4f8b\u4ecb\u7ecd\uff1a\u6c42\u52a9\u8005\u7559\u5b66\u5f52\u6765\u8fdb\u5165\u67d0\u79d1\u7814\u5355\u4f4d\u2026\u2026\u534a\u5e74\u524d\u88ab\u5176\u4ed6 \u540c\u4e8b\u6279\u8bc4\uff0c\u611f\u5230\u5f88\u751f\u6c14\uff0c\u8ba4\u4e3a\u522b\u4eba\u4e0d\u5e94\u8be5\u4e0e\u81ea\u5df1\u8ba1\u8f83\u2026\u2026\u56e0\u6b64\uff0c \u60f3\u5230\u56fd\u5916\u5de5\u4f5c\u3002\u4f46\u662f\uff0c\u6c42\u52a9\u8005\u4e08\u592b\u4e0d\u613f\u51fa\u56fd\uff0c\u5bb6\u5ead\u5f00\u59cb\u51fa\u73b0\u77db\u76fe\uff0c \u6c42\u52a9\u8005\u5185\u5fc3\u75db\u82e6\u3001\u60c5\u7eea\u4e0d\u597d\uff0c\u4f53\u91cd\u4e0b\u964d\u2026\u2026\u7ecf\u4f53\u68c0\uff0c\u6ca1\u6709\u53d1\u73b0\u5668 \u8d28\u6027\u75c5\u53d8\u2026\u2026\u95ee\u9898\uff1a\u8be5\u6c42\u52a9\u8005\u5fc3\u7406\u51b2\u7a81\u7684\u7c7b\u578b\u5305\u62ec\uff08 \uff09\u3002A. \u53d8\u5f62 B. \u8d8b\u907f\u5f0fC. \u5e38\u5f62D. \u53cc\u8d8b\u5f0f General Information: Client, female, 36 years old, PhD, researcher. Case Introduction: The client returned from studying abroad and joined a research institution... Six months ago, she was criticized by colleagues and felt very angry, believing that others should not be so petty with her... Therefore, she considered working abroad. However, her husband was unwilling to go broad, leading to marital conflicts. The client experienced inner distress and emotional difficulties, with weight loss... Physical examination revealed no organic pathology... Question: The type(s) of psychological conflict include: A. Neurotic B. Approach-avoidance C. Normal D. Approach-approach Psyche-R1 <think> \u2026\u2026\u8fd9\u79cd\u5fc3\u7406\u51b2\u7a81\u7684\u7279\u70b9\u5728\u4e8e\u6c42\u52a9\u8005\u5728 \u8ffd\u6c42\u67d0\u4e00\u76ee\u6807\uff08\u51fa\u56fd\u5de5\u4f5c\uff09\u7684\u540c\u65f6\uff0c\u53c8\u53d7\u5230\u53e6\u4e00\u76ee\u6807\uff08\u7ef4\u62a4\u5bb6 \u5ead\u5173\u7cfb\uff09\u7684\u963b\u788d\uff0c\u5f62\u6210\u4e86\u8d8b\u907f\u5f0f\u5fc3\u7406\u51b2\u7a81\u2026\u2026\u7b26\u5408\u5e38\u5f62\u51b2\u7a81\uff0c \u56e0\u4e3a\u6c42\u52a9\u8005\u5728\u9762\u5bf9\u73b0\u5b9e\u95ee\u9898\u65f6\uff0c\u80fd\u591f\u6e05\u6670\u5730\u8ba4\u8bc6\u5230\u95ee\u9898\u7684\u5b58\u5728\uff0c \u5e76\u8bd5\u56fe\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u2026\u2026\u4e5f\u7b26\u5408\u8d8b\u907f\u5f0f\u51b2\u7a81\uff0c\u56e0\u4e3a\u6c42\u52a9\u8005\u5728\u8ffd \u6c42\u67d0\u4e00\u76ee\u6807\uff08\u51fa\u56fd\u5de5\u4f5c\uff09\u7684\u540c\u65f6\uff0c\u53c8\u53d7\u5230\u53e6\u4e00\u76ee\u6807\uff08\u7ef4\u62a4\u5bb6\u5ead \u5173\u7cfb\uff09\u7684\u963b\u788d\uff0c\u5f62\u6210\u4e86\u8d8b\u907f\u5f0f\u5fc3\u7406\u51b2\u7a81\u3002</think>\u7b54\u6848\uff1aBC <think> ...The characteristic of this psychological conflict is that the client, while pursuing one goal (working abroad), is simultaneously hindered by another goal (maintaining family relationships), forming an approach-avoidance conflict... It fits the pattern of normal conflict, because when facing real-world problems, the client can clearly recognize the existence of the problem and attempts to find solutions. It also fits the approach-avoidance conflict pattern, because the client, while pursuing one goal (working abroad), is simultaneously hindered by another goal (maintaining family relationships), forming an approach-avoidance psychological conflict. </think> Answer: BC Qwen2.5-72B-Instruct \u8be5\u6c42\u52a9\u8005\u7684\u5fc3\u7406\u51b2\u7a81\u4e3b\u8981\u8868\u73b0\u4e3a\u5bf9\u51fa\u56fd\u5de5\u4f5c\u7684\u5f3a\u70c8\u613f\u671b\u4e0e\u5bb6\u5ead \u56e0\u7d20\uff08\u5982\u4e08\u592b\u4e0d\u613f\u51fa\u56fd\uff09\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u56e0\u6b64\u5c5e\u4e8e\u5178\u578b\u7684\u8d8b\u907f\u5f0f \u51b2\u7a81\u3002\u6b64\u5916\uff0c\u6c42\u52a9\u8005\u8ba4\u4e3a\u540c\u4e8b\u5e94\u8be5\u4ee5\u79d1\u7814\u4e3a\u91cd\uff0c\u5374\u5728\u610f\u522b\u4eba\u5bf9 \u81ea\u5df1\u6001\u5ea6\u7684\u770b\u6cd5\uff0c\u8fd9\u663e\u793a\u51fa\u5979\u7684\u8ba4\u77e5\u51fa\u73b0\u4e86\u53d8\u5f62\uff0c\u5bfc\u81f4\u4e86\u5185\u5fc3 \u7684\u77db\u76fe\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u8be5\u6c42\u52a9\u8005\u5fc3\u7406\u51b2\u7a81\u7684\u7c7b\u578b\u5305\u62ec\uff1aB. \u8d8b\u907f\u5f0f A. \u53d8\u5f62 \u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f B \u548c A\u3002 This client's psychological conflict is primarily manifested in the contradiction between their strong desire to work abroad and family factors (such as their husband's unwillingness to go abroad), which represents a typical approach-avoidance conflict. Additionally, the client believes that",
                "metadata": {
                    "paper_title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms",
                    "filename": "papers/paper_4.pdf",
                    "chunk_index_in_paper": 10
                }
            },
            {
                "distance": 1.0351636409759521,
                "chunk": "(Zheng et al. 2023) improves emotional sensitivity in dialogue sys- tems by incorporating an emotion-aware attention mech- anism. However, these approaches often lack the exper- tise foundation required for psychology, which is impor- tant for accurate psychological understanding. Some stud- ies have attempted to address this limitation through inte- gration of psychological knowledge (Chen, Lu, and Wang 2023; Xiao et al. 2024; Wu, Huang, and Lu 2025). For ex- ample, PsycoLLM (Hu et al. 2024) integrates psychologi- cal knowledge by training its model on knowledge-based question-answer (QA) pairs, while CPsyExam (Zhao et al. 2025) leverages examination questions covering theoreti- cal knowledge from different psychology-related subjects to further improve model performance. Although existing stud- ies have achieved considerable success, they remain limited in their capacity for complex reasoning. In fact, reasoning- augmented LLMs trained through reinforcement learning (RL) have demonstrated superior performance across vari- ous domains, particularly in mathematics, code generation, and medical domain (Chen et al. 2024; Guo et al. 2025). However, as shown in Figure 1, these reasoning-augmented LLMs exhibit limited performance in the psychological do- arXiv:2508.10848v1 [cs.CL] 14 Aug 2025 No Emoji/ Emoticons Consistent Punctuation No Redun- dant Content No Ads/Links Data Cleaning Data Hello, you've described symptoms of insomnia, chest tightness, dizziness\u2026 If there is a hell, I feel you would be that ray of light in hell. Your powerful awareness and reflection have \u2026 It seems like you feel quite guilty and have some self-awareness. For you\u2026 I can really sense how difficult this must be for you right now. Dealing with\u2026 I can hear how much distress you are carrying right now. It sounds like\u2026 Behavioral Psychology My heart goes out to you for everything you've been through. To have survived what felt like hell... I can understand\u2026 Empathetic Dialogue Synthesis Question Generation and Control Applied Psychology Abnormal Psychology Maladaptive behavior is a key... This includes actions interfere with long- term goals\u2026 and are ultimately self- defeating, preventing an individual from adapting to new or difficult circumstances. It's a core\u2026 \u2026 LLM \ufffd\ufffd : A student consistently avoids studying for exams, leading to poor grades and academic probation. This is.. A. xx B. yy C. zz D. kk\u2026 Answer: C. Maladaptive behavior \ufffd\ufffd : Which scenario best illustrates a behavior's role\u2026 A. xx B. yy C. zz D. kk\u2026 Answer: B. An individual who fearing failure avoids.. \u2026 \ufffd\ufffd: A behavior is defined as maladaptive primarily\u2026 MinHash LSH LLM Filter Diversity Human Reviewer Quality PQA QA Pairs (\ufffd\ufffd, \ufffd\ufffd, \ufffd\ufffd) \ufffd\ufffd Prompt (\ufffd, \ufffd\ufffd, \ufffd\ufffd) if \ufffd\ufffd \ufffd \ufffd\ufffd = Initial Instance (\ufffd, \ufffd\ufffd, \ufffd\ufffd) Candidate \ufffd\ufffd \u2217 if \ufffd\ufffd \u2217 \ufffd\ufffd = \ufffd\ufffd (\ufffd\ufffd \u2217, \ufffd\ufffd \u2217) Update (\ufffd\ufffd \u2217, \ufffd\ufffd, \ufffd\ufffd \u2217) Revert Previous if not Iterate for \ufffd Rounds Rationale Generation Correct\u22651 All Incorrect Question Selection Model Training Psyche-R1 Base Model SFT SFT Model GRPO (SFT) (GRPO) Multi-LLM Selector \u2026\u2026 Figure 2: Overview of our proposed pipeline for constructing the dataset and Psyche-R1. Our pipeline involves generating psychological questions paired with detailed rationales, along with empathetic dialogues. main, since they focus on logic reasoning,",
                "metadata": {
                    "paper_title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms",
                    "filename": "papers/paper_4.pdf",
                    "chunk_index_in_paper": 1
                }
            },
            {
                "distance": 1.0714006423950195,
                "chunk": "on established psychological therapies (Lee et al. 2024; Shen et al. 2024) or concepts (Zhang et al. 2025). LLM Reasoning. In recent years, techniques such as CoT prompting (Wei et al. 2022; Hsieh et al. 2023) have signifi- cantly advanced the development of LLM reasoning. Build- ing upon this foundation, researchers have explored more sophisticated reasoning architectures. For instance, Tree of Thoughts (Yao et al. 2023) enables systematic exploration of multiple reasoning paths with self-evaluation, while PAL (Gao et al. 2023) integrates reasoning with external tools through program generation. These approaches further en- hance model performance in handling complex tasks. A new breakthrough was marked by the release of reasoning LLMs like OpenAI o1 (Jaech et al. 2024) and DeepSeek-R1 (Guo et al. 2025). These models, which are trained through reinforcement learning to enhance reasoning capabilities, demonstrate exceptional performance in mathematical and coding tasks (Comanici et al. 2025; Yang et al. 2025). Mo- tivated by these advances, researchers have employed ad- vanced RL algorithms, such as GRPO (Shao et al. 2024) and DAPO (Yu et al. 2025), to extend reasoning capabilities to domain-specific applications, including medicine (Liu et al. 2025) and finance (Zhu et al. 2025). However, within the field of psychology, limited research has investigated the utility of reasoning. To our knowledge, Psyche-R1 is the first psychological LLM that unifies empathy, domain-specific expertise, and reasoning capabilities. Conclusion In this paper, we propose Psyche-R1, the first Chinese psy- chological LLM that jointly integrates empathy, expertise, and reasoning. To support model development, we design a multi-stage data synthesis pipeline that generates high- quality psychological reasoning samples with detailed ra- tionales and empathetic dialogues. The reasoning rationales are further enhanced through iterative prompt\u2013rationale op- timization, and a multi-LLM cross-selection strategy is em- ployed to identify challenging examples. Finally, the chal- lenging subset is used for GRPO, while the remaining data are employed for SFT, together contributing to the final model. Extensive experiments demonstrate that Psyche-R1 outperforms existing psychological LLMs, achieving perfor- mance comparable to DeepSeek-R1. References Al Asad, N.; Pranto, M. A. M.; Afreen, S.; and Islam, M. M. 2019. Depression detection by analyzing social media posts of user. In 2019 IEEE international conference on signal processing, information, communication & systems (SPIC- SCON), 13\u201317. IEEE. Chen, J.; Cai, Z.; Ji, K.; Wang, X.; Liu, W.; Wang, R.; Hou, J.; and Wang, B. 2024. Huatuogpt-o1, towards medical com- plex reasoning with llms. arXiv preprint arXiv:2412.18925. Chen, Y.; Xing, X.; Lin, J.; Zheng, H.; Wang, Z.; Liu, Q.; and Xu, X. 2023. SoulChat: Improving LLMs\u2019 Empathy, Listen- ing, and Comfort Abilities through Fine-tuning with Multi- turn Empathy Conversations. In Findings of the Association for Computational Linguistics: EMNLP 2023, 1170\u20131183. Chen, Z.; Lu, Y.; and Wang, W. 2023. Empowering Psy- chotherapy with Large Language Models: Cognitive Distor- tion Detection through Diagnosis of Thought Prompting. In Findings of the Association for Computational Linguistics: EMNLP 2023, 4295\u20134304. Cho, Y.; Kim, M.; Kim, S.; Kwon, O.; Kwon, R. D.; Lee, Y.; and Lim, D. 2023. Evaluating the efficacy of interac- tive language therapy based on LLM for high-functioning autistic adolescent",
                "metadata": {
                    "paper_title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms",
                    "filename": "papers/paper_4.pdf",
                    "chunk_index_in_paper": 12
                }
            }
        ]
    },
    {
        "query": "what is the result of human evaluation between VAC and the best performing baseline?",
        "results": [
            {
                "distance": 0.8414674997329712,
                "chunk": "0.5331\u2020 0.4691\u2020 Figure 5: Performance of VAC in different training iterations with trained and untrained feedback model. training. The model is trained for three iterations under the same configuration as our method, with the best checkpoint from all iterations used for evaluation. This comparison enables an empir- ical assessment of the efficiency and effectiveness of NLF-based optimization versus scalar reward-based optimization. All baselines are evaluated under the same setup and conditions as VAC, including identical configurations for maximum input and output lengths, training budget, number of retrieved documents, retrieval model, and generation temperature. 5.2 Empirical Results Comparison with the Baselines: The results of our method and the baselines on the LaMP-QA benchmark datasets are presented in Table 2. As shown, VAC statistically significantly outperforms all baselines in terms of average performance across the benchmark. More specifically, VAC achieves statistically significant improve- ments over all baselines in 2 out of the 3 personalized question answering tasks. The only task where VAC does not outperform the baselines is Art & Entertainment. These results highlight the effec- tiveness of learning from natural language feedback for improving personalization in response generation based on user preferences. Table 2 reports the runtime for each method. Among them, the non-personalized LLM yields the lowest runtime, primarily because it processes shorter inputs and incurs no retrieval overhead. In contrast, all RAG-based personalization methods\u2014including VAC\u2014 have higher runtime due to the added cost of retrieving relevant user profile documents. The highest runtime is observed for the PlanPers baseline [30], which is nearly twice as slow as VAC due to two step generation method used in this method, yet yields significantly lower performance. Overall, these results demonstrate that VAC provides superior personalization performance with runtime costs comparable to the most efficient personalized baselines. Effect of Optimizing Feedback Model: To examine the effect of training the feedback model on the performance of VAC, we con- duct two sets of experiments: one in which both the policy and feedback models are updated after each iteration, and another in which the feedback model remains frozen while only the policy model is trained. The results are reported in Figure 5. As shown, jointly training the feedback model to align with the evolving policy model consistently outperforms the frozen-feedback setup across all datasets. These findings highlight the importance of optimizing the feedback model in each iteration to match the updated capabili- ties of the policy model, thereby enabling the generation of more effective and informative feedback. Effect of Number of Training Iterations (\ud835\udc47): To investigate the impact of the number of training iterations (\ud835\udc47) on the performance of VAC, we train the model for up to three iterations and evaluate it after each iteration. The results in Figure 5 show that the perfor- mance improves during the first two iterations but plateaus in the third. It also indicates that this plateau effect is more pronounced when the feedback model is untrained, highlighting the importance of optimizing the feedback model on the performance. These obser- vations suggest that continued training with VAC yields diminishing returns after a few",
                "metadata": {
                    "paper_title": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs",
                    "filename": "papers/paper_10.pdf",
                    "chunk_index_in_paper": 13
                }
            },
            {
                "distance": 1.0284126996994019,
                "chunk": "systems generate com- plete peer reviews where novelty is a minor com- ponent. OpenReviewer performs worst, lacking re- trieval and relying on parametric knowledge. Deep- Reviewer uses OpenScholar retrieval but fails at comparative analysis. Human reviewers show high variance, with some engaging extensively while others provide minimal analysis. 5.4 Human Evaluation Validation We conducted human evaluations to validate our LLM-as-Judge evaluation framework. Figures 3 and 4 show the pairwise comparison results. Against OpenReviewer, our system wins 74% of the time. Performance against DeepReviewer and human reviewers is more mixed (39% and 36% win rates), but high tie rates (30% and 41%) indicate many assessments were judged comparable. Loss rates remain low across all comparisons (16-26%). By dimension (Figure 4), Claim Substantiation and Analytical Quality perform best (56% and 55% win rates). Novelty Decision shows the most ties (31%), suggesting different approaches often yield similar conclusions. These patterns align with our auto- mated results, supporting our evaluation approach\u2019s validity. 5.5 Analysis: Understanding Human Alignment Patterns Our system\u2019s higher agreement scores compared to human-human baselines warrant careful exam- ination. To investigate this, we analyzed papers with multiple human reviewers to understand the sources of disagreement. 5.5.1 Sources of Human Reviewer Variability Qualitative analysis reveals several factors con- tributing to reviewer disagreement: Different Eval- uation Lenses: Reviewers often focus on different aspects of novelty. In submission Ipe4fMCBXk, half the reviewers emphasized methodological con- tributions while others focused on application nov- elty, leading to opposite conclusions from the same paper. Varying Domain Expertise: Reviewers\u2019 background knowledge affects assessments. For instance, in a protein design paper, reviewers famil- iar with the field\u2019s history correctly identified prior work on recombination techniques, while others assessed these as novel contributions. Assessment Granularity: Some reviewers provide high-level judgments (\"innovative approach\") while others fo- cus on specific technical details. This variation in granularity contributes to disagreement even when reviewers might agree on underlying facts. 5.5.2 The Role of Systematic Evaluation Our system\u2019s approach differs from human review in applying consistent evaluation criteria. It eval- uates multiple dimensions (methodology, applica- tion, prior work) for every paper, maintains uniform depth of analysis across assessments, and applies consistent thresholds for novelty judgments. This systematic approach may explain the alignment patterns: when human reviewers disagree due to focusing on different aspects, our system\u2019s com- prehensive evaluation can align partially with each perspective. 5.6 Component Analysis Table 5 shows the incremental contribution of each pipeline component. Our human-informed prompt design provides the largest gains (+40.7% reason- ing, +46.8% conclusion), reflecting the importance of structured evaluation criteria derived from our human analysis. Structured extraction adds moder- 8 ate improvements (+3.3% reasoning, +4.5% con- clusion) but reduces overall computation costs and time by a lot, while landscape analysis contributes minimally (+3.2% reasoning, -0.7% conclusion). 6 Conclusion We present a human-informed pipeline for auto- mated novelty assessment in peer review, address- ing a critical gap in AI-assisted review systems. Our approach combines systematic related work retrieval with structured evaluation criteria derived from analysis of expert reviewer patterns. Experi- mental results demonstrate that our system outper- forms existing AI baselines and",
                "metadata": {
                    "paper_title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions",
                    "filename": "papers/paper_7.pdf",
                    "chunk_index_in_paper": 9
                }
            },
            {
                "distance": 1.0804054737091064,
                "chunk": "struc- tured JSON files containing dimension-wise selec- 11 tions (A/B/Tie/Unclear), time spent per evaluation, and comments for flagged cases. Category Agreement Kappa Comparisons Novelty Reasoning Alignment 0.520 0.341 75 Novelty Decision Alignment 0.533 0.346 75 Claim Substantiation 0.493 0.287 75 Analytical Quality 0.560 0.368 75 Table 6: Inter-Rater Reliability Metrics Across Cate- gories B Output Examples Output of our pipeline can be seen in Tables 7, 8 and 9. It is quite evident that our system aligns better with the human as compared to the baselines across all four dimensions. Figure 4: Performance breakdown across evaluation categories, aggregated across all baseline comparisons. Figure 5: Distribution of the number of reviews per paper. Most papers received 1 to 4 reviews. Figure 6: Screenshot of the custom-built interface used for human evaluation. Annotators compared AI-generated and human-written novelty assessments across multiple dimensions, including reasoning depth, prior work engagement, and conclusion alignment. Figure 7: Screenshot (2) of the custom-built inter- face used for human evaluation. Annotators compared AI-generated and human-written novelty assessments across multiple dimensions, including reasoning depth, prior work engagement, and conclusion alignment. 12 Human (Reference) Scideator (Baseline) Ours (Proposed) The proposed approach aims to significantly reduce memory consumption during long-sequence inference while maintaining model performance and requiring minimal modifications to existing LLM frameworks. While the integration of CGE and RGL is presented as a novel solution to the KV cache memory challenge, the overall novelty of the work is limited . Sparse attention mechanisms have already been extensively explored in prior liter- ature, such as [1] and [2] , which diminishes the originality of the proposed methods. Furthermore, the CGE component closely resembles previous ap- proaches like H20 [3] and SnapKV [4], with only incremental differ- ences . The feedback mechanism using sliding windows has also been well-studied in H20 [3]. As such, the main contribution appears to be a specific combination and implementation of existing ideas rather than a fundamentally new technique . The paper would benefit from a clearer articulation of how its methods differ from these established approaches to better establish its unique contribution. - Class: novel The idea is novel because IntelLLM introduces a distinct approach to key-value (KV) cache compression by using strategies like center of gravity eviction (CGE) and remote gap localization (RGL), which are not seen in existing works such as RazorAttention[o] and LeanKV[1]. These methods prioritize retaining essential tokens and leveraging positional features to enhance compression efficiency, offering a dedicated approach with unique techniques for balancing compression rate and performance. IntelLLM introduces new heuristics\u2014Center of Gravity Eviction (CGE) and Remote Gap Localization (RGL)\u2014for token selection and long-range dependency preservation in KV cache compression for LLM inference, but these are incremental variants of established token eviction approaches . The submission overstates its novelty , as the core ideas (token selection, attention spar- sity, training-free deployment) are already well-explored , and similar methods (e.g., RazorAttention, PyramidKV, L2 Norm) achieve comparable goals without model changes or fine-tuning. Several highly relevant recent works are omitted from the discussion , and the claims of being the first to balance com-",
                "metadata": {
                    "paper_title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions",
                    "filename": "papers/paper_7.pdf",
                    "chunk_index_in_paper": 14
                }
            }
        ]
    }
]